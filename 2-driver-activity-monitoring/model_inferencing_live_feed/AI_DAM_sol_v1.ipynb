{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGIsF1ADyJ58"
   },
   "source": [
    "# AI based driver activity monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "#model_path = \"models/vgg16_size128_best.h5\"\n",
    "model_path = \"models/restnet50_size128_best.h5\" \n",
    "\n",
    "camera_port = 0 # Camera 0 is the integrated web cam on my netbook\n",
    "#camera_port = 1 # For usb camera\n",
    "\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "left_data_dir = 'D:/my_learning_dataset/state_farms_drivers/imgs/test/'\n",
    "right_data_dir = 'D:/my_learning_dataset/state_farms_drivers/imgs_right/test/'\n",
    "\n",
    "data_dir = right_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>left_desc</th>\n",
       "      <th>right_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0</td>\n",
       "      <td>safe-driving</td>\n",
       "      <td>safe-driving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c1</td>\n",
       "      <td>texting-right</td>\n",
       "      <td>texting-left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c2</td>\n",
       "      <td>talking-phone-right</td>\n",
       "      <td>talking-phone-left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c3</td>\n",
       "      <td>texting-left</td>\n",
       "      <td>texting-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c4</td>\n",
       "      <td>talking-phone-left</td>\n",
       "      <td>talking-phone-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c5</td>\n",
       "      <td>operating-radio</td>\n",
       "      <td>operating-radio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c6</td>\n",
       "      <td>drinking</td>\n",
       "      <td>drinking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c7</td>\n",
       "      <td>reaching-behind</td>\n",
       "      <td>reaching-behind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c8</td>\n",
       "      <td>hair-and-makeup</td>\n",
       "      <td>hair-and-makeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c9</td>\n",
       "      <td>talking-to-passenger</td>\n",
       "      <td>talking-to-passenger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class             left_desc            right_desc\n",
       "0    c0          safe-driving          safe-driving\n",
       "1    c1         texting-right          texting-left\n",
       "2    c2   talking-phone-right    talking-phone-left\n",
       "3    c3          texting-left         texting-right\n",
       "4    c4    talking-phone-left   talking-phone-right\n",
       "5    c5       operating-radio       operating-radio\n",
       "6    c6              drinking              drinking\n",
       "7    c7       reaching-behind       reaching-behind\n",
       "8    c8       hair-and-makeup       hair-and-makeup\n",
       "9    c9  talking-to-passenger  talking-to-passenger"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is original image dataset for left hand driving\n",
    "class_list =  ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6','c7', 'c8', 'c9']\n",
    "left_class_desc = ['safe-driving', 'texting-right', 'talking-phone-right', 'texting-left', 'talking-phone-left', \n",
    "              'operating-radio', 'drinking', 'reaching-behind', 'hair-and-makeup', 'talking-to-passenger']\n",
    "right_class_desc = ['safe-driving', 'texting-left', 'talking-phone-left', 'texting-right', 'talking-phone-right', \n",
    "              'operating-radio', 'drinking', 'reaching-behind', 'hair-and-makeup', 'talking-to-passenger']\n",
    "df_desc = pd.DataFrame({'class': class_list, 'left_desc': left_class_desc,  'right_desc': right_class_desc})\n",
    "df_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_desc = right_class_desc\n",
    "#class_desc = left_class_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getActivity(predictions, threshold=0.50):\n",
    "    result = np.where(predictions[0] == np.max(predictions[0])) # this is a list\n",
    "    pos = result[0][0] # this is max probability\n",
    "    if(predictions[0][pos] >= threshold):\n",
    "        return pos, class_desc[pos], predictions[0][pos]\n",
    "    else:\n",
    "        return 10, \"no class\", 0\n",
    "    \n",
    "def getActivityList(predictions, top=3):\n",
    "    val = sorted(zip(predictions[0], class_desc), reverse=True)[:top]\n",
    "    return val\n",
    "\n",
    "def getTextForDisplay(actlist):\n",
    "    text = \"\";\n",
    "    for idx, pair in enumerate(actlist):    \n",
    "        text = text + \"{}: {:0.2f}\".format(pair[1], pair[0]) + \"\\n\"\n",
    "    text = text[:-1]\n",
    "    return text\n",
    "\n",
    "def plotImageWithActivityPredictions(file, actlist):\n",
    "    displayText = getTextForDisplay(actlist)\n",
    "    img = cv2.imread(file)\n",
    "    plt.text(x=0, y=0,s=displayText, \n",
    "         bbox=dict(facecolor='orange', alpha=0.5), \n",
    "         horizontalalignment='left', \n",
    "         verticalalignment='top',\n",
    "         fontsize=10)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "def plotImageWithActivityPredictionsExt(img, actlist):\n",
    "    displayText = getTextForDisplay(actlist)\n",
    "    plt.text(x=0, y=0,s=displayText, \n",
    "         bbox=dict(facecolor='orange', alpha=0.5), \n",
    "         horizontalalignment='left', \n",
    "         verticalalignment='top',\n",
    "         fontsize=10)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img_cv2(filepath, size):\n",
    "    img = cv2.imread(filepath) #, cv2.IMREAD_GRAYSCALE\n",
    "    img = cv2.resize(img, size, interpolation = cv2.INTER_AREA) # resize image  \n",
    "    img = image.img_to_array(img)\n",
    "    return img\n",
    "\n",
    "# input is image file\n",
    "def cv2_pre_processing(filepath, size):\n",
    "    img = read_img_cv2(filepath, size)\n",
    "    img_preprocessed = np.expand_dims(img.copy(), axis=0)\n",
    "    img_preprocessed = img_preprocessed.astype('float32')/255\n",
    "    return img_preprocessed\n",
    "\n",
    "# input is image data\n",
    "def cv2_pre_processing_data(iframe, size):\n",
    "    img = cv2.resize(iframe, size, interpolation = cv2.INTER_AREA) # resize image  \n",
    "    img = image.img_to_array(img)\n",
    "    img_preprocessed = np.expand_dims(img.copy(), axis=0)\n",
    "    img_preprocessed = img_preprocessed.astype('float32')/255\n",
    "    return img_preprocessed\n",
    "\n",
    "def cv2_model_execution(model, filepath, size, threshold=0.5, top=3):\n",
    "    preproc = cv2_pre_processing(filepath, size)\n",
    "    preds = model.predict(preproc)\n",
    "    actlist = getActivityList(preds,top)\n",
    "    plotImageWithActivityPredictions(file,actlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using image library of keras\n",
    "def read_img_grayscale(filepath, size):\n",
    "    img = image.load_img(filepath, target_size=size, color_mode=\"grayscale\")\n",
    "    img_data = image.img_to_array(img)\n",
    "    return img_data\n",
    "\n",
    "# input is image file\n",
    "def pre_processing(filepath, size):\n",
    "    img = read_img_grayscale(filepath, size)\n",
    "    img_preprocessed = np.expand_dims(img.copy(), axis=0)\n",
    "    img_preprocessed = img_preprocessed.astype('float32')/255\n",
    "    return img_preprocessed\n",
    "\n",
    "# input is image data\n",
    "def pre_processing_data(iframe, size):\n",
    "    img = np.resize(iframe, new_shape=size)\n",
    "    img = image.img_to_array(img)\n",
    "    img_preprocessed = np.expand_dims(img.copy(), axis=0)\n",
    "    img_preprocessed = img_preprocessed.astype('float32')/255\n",
    "    return img_preprocessed\n",
    "    \n",
    "def model_execution(model, filepath, size, threshold=0.5, top=3):\n",
    "    preproc = pre_processing(filepath, size)\n",
    "    preds = model.predict(preproc)\n",
    "    actlist = getActivityList(preds,top)\n",
    "    plotImageWithActivityPredictions(file,actlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def vgg_read_image_cv2(filepath, size):\n",
    "    img = cv2.imread(filepath) #, cv2.IMREAD_GRAYSCALE\n",
    "    img = cv2.resize(img, size, interpolation = cv2.INTER_AREA) # resize image  \n",
    "    img = image.img_to_array(img) # convert the image pixels to a numpy array\n",
    "    # Convert the image / images into batch format\n",
    "    # expand_dims will add an extra dimension to the data at a particular axis\n",
    "    # We want the input matrix to the network to be of the form (batchsize, height, width, channels)\n",
    "    # Thus we add the extra dimension to the axis 0.\n",
    "    img = np.expand_dims(img, axis=0) # reshape data for the model\n",
    "    return img\n",
    "\n",
    "def vgg_read_image(filepath, size):\n",
    "    img = vgg_read_image_cv2(filepath, size)\n",
    "    img_preprocessed = preprocess_input(img.copy()) # # prepare the image for the VGG model\n",
    "    img_preprocessed = img_preprocessed.astype('float32')/255\n",
    "    return img_preprocessed\n",
    "\n",
    "def vgg_pre_processing_data(iframe, size):  \n",
    "    img = cv2.resize(iframe, size, interpolation = cv2.INTER_AREA) # resize image  \n",
    "    img = image.img_to_array(img) # convert the image pixels to a numpy array\n",
    "    # Convert the image / images into batch format\n",
    "    # expand_dims will add an extra dimension to the data at a particular axis\n",
    "    # We want the input matrix to the network to be of the form (batchsize, height, width, channels)\n",
    "    # Thus we add the extra dimension to the axis 0.\n",
    "    img = np.expand_dims(img, axis=0) # reshape data for the model\n",
    "    img_preprocessed = preprocess_input(img.copy()) # # prepare the image for the VGG model\n",
    "    img_preprocessed = img_preprocessed.astype('float32')/255\n",
    "    return img_preprocessed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For VGG model\n",
    "def dam_read_image(filepath, size):\n",
    "    img = vgg_read_image(filepath, size) # for VGG\n",
    "    #img = cv2_pre_processing(filepath, size)  # for custom\n",
    "    return img\n",
    "\n",
    "def dam_pre_processing_data(iframe, size):\n",
    "    img = vgg_pre_processing_data(iframe, size) # for VGG\n",
    "    #img = cv2_pre_processing_data(iframe, size) # for custom\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Restnet model\n",
    "def dam_read_image(filepath, size):\n",
    "    #img = vgg_read_image(filepath, size) # for VGG\n",
    "    img = cv2_pre_processing(filepath, size)  # for restnet50 and custom\n",
    "    return img\n",
    "\n",
    "def dam_pre_processing_data(iframe, size):\n",
    "    #img = vgg_pre_processing_data(iframe, size) # for VGG\n",
    "    img = cv2_pre_processing_data(iframe, size) # for restnet50 and custom\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "trainedModel = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 4, 4, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 57,673,098\n",
      "Trainable params: 57,619,978\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainedModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***image file prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_prediction_for_test_file(testfile):\n",
    "    preprocessedInp = dam_read_image(testfile,IMAGE_SIZE)\n",
    "    preds = trainedModel.predict(preprocessedInp)\n",
    "\n",
    "    pos, classname, prob = getActivity(preds, threshold=0.5)\n",
    "    #print(\"{}, {:0.2f}\".format(classname, prob))\n",
    "\n",
    "    actList = getActivityList(preds,top=3)\n",
    "    print(actList)\n",
    "\n",
    "    plotImageWithActivityPredictions(testfile,actList)\n",
    "    return"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for right hand driving\n",
    "test_dir = 'D:/my_learning_dataset/state_farms_drivers/imgs_right/test'\n",
    "\n",
    "filenum =  100004\n",
    "test_image_path = '{}/img_{}.jpg'.format(test_dir, filenum)\n",
    "\n",
    "activity_prediction_for_test_file(test_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Live video, get frame in real time and predict activity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAM_live_monitoring():\n",
    "\n",
    "    #Number of frames to throw away while the camera adjusts to light levels\n",
    "    ramp_frames = 30\n",
    "\n",
    "    try:\n",
    "\n",
    "        print(\"opening camera and cv2 resources\")\n",
    "        cap = cv2.VideoCapture(camera_port)\n",
    "        \n",
    "        print(\"camera warm ups\")\n",
    "        # camera warm up before we begin looping over the frames\n",
    "        time.sleep(2)\n",
    "\n",
    "        #Check whether user selected camera is opened successfully.\n",
    "        if not (cap.isOpened()):\n",
    "            print(\"Could not open video device\")\n",
    "\n",
    "        #To set the resolution\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  #640\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) # 480\n",
    "\n",
    "        #cv2.resizeWindow(\"Driver Activity Detection\", 800, 600)\n",
    "\n",
    "        # Grab the frame continuously from the camera and show it in the preview window using the while loop. \n",
    "        # Enter ‘q’ key, to break the loop and exit the application\n",
    "        print(\"Enter ‘q’ key, to break the loop and exit the application\")\n",
    "        \n",
    "        prev_classid = 0\n",
    "        counter=0\n",
    "        while(True):\n",
    "\n",
    "            #time.sleep(0.1)\n",
    "\n",
    "            #for i in range(ramp_frames):    \n",
    "            #    ret, frame = cap.read()    # Capture frame-by-frame\n",
    "            #    cv2.imshow('Driver Activity Detection',frame)\n",
    "\n",
    "\n",
    "            # image to predict activity\n",
    "            ret, frame = cap.read()  \n",
    "            #frame = cv2.flip(frame1,1)\n",
    "\n",
    "            processedImg = dam_pre_processing_data(frame, IMAGE_SIZE)\n",
    "            preds = trainedModel.predict(processedImg)\n",
    "            pos, classname, prob = getActivity(preds, 0.2)\n",
    "            #print(\"{},{}\".format(classname, prob))\n",
    "\n",
    "            actlist = getActivityList(preds,top=3)\n",
    "            displayText = getTextForDisplay(actlist)\n",
    "            \n",
    "            if(prev_classid == pos):\n",
    "                counter += 1\n",
    "            else:\n",
    "                counter = 0\n",
    "                \n",
    "            prev_classid = pos\n",
    "                \n",
    "            if(counter > 5):\n",
    "                # Display the resulting frame\n",
    "                label = \"{}:{:0.2f}\".format(classname, prob)\n",
    "            else:\n",
    "                label = \"no class\"\n",
    "\n",
    "            #dispframe = cv2.putText(frame, label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            dispframe = cv2.putText(frame, label, \n",
    "                                    (10, 25), # bottomLeftCornerOfText\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, # font\n",
    "                                    0.7, # fontScale\n",
    "                                    (0, 255, 0), #fontColor\n",
    "                                    2 # lineType\n",
    "                                    )\n",
    "            cv2.imshow('Driver Activity Monitoring',dispframe) # show the output frame\n",
    "\n",
    "            #Waits for a user input to quit the application\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            # if the `q` key was pressed, break from the loop\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "    finally:        \n",
    "        # Release the camera, then close all of the imshow() windows\n",
    "        # When everything done, release the capture\n",
    "        print(\"Releasing camera and cv2 resources\")\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening camera and cv2 resources\n",
      "camera warm ups\n",
      "Enter ‘q’ key, to break the loop and exit the application\n",
      "Releasing camera and cv2 resources\n"
     ]
    }
   ],
   "source": [
    "DAM_live_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single click prediction using live camera**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dam_one_shot_live_prediction():   \n",
    "\n",
    "    #Number of frames to throw away while the camera adjusts to light levels\n",
    "    ramp_frames = 30\n",
    "\n",
    "    try:\n",
    "        print(\"opening camera and cv2 resources\")\n",
    "        cap = cv2.VideoCapture(camera_port)\n",
    "\n",
    "        #Check whether user selected camera is opened successfully.\n",
    "        if not (cap.isOpened()):\n",
    "            print(\"Could not open video device\")\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        #frame = cv2.flip(frame1,1)\n",
    "        #print(\"Frame: \", frame.shape)\n",
    "\n",
    "        processedImg = dam_pre_processing_data(frame,IMAGE_SIZE)\n",
    "        preds = trainedModel.predict(processedImg)\n",
    "        #pos, classname, prob = getActivity(preds, 0.001)\n",
    "        #print(\"{},{}\".format(classname, prob))\n",
    "\n",
    "        actlist = getActivityList(preds,top=3)\n",
    "        plotImageWithActivityPredictionsExt(frame,actlist)\n",
    "\n",
    "        cv2.imwrite(outfile, frame)\n",
    "\n",
    "    finally:        \n",
    "        # Release the camera, then close all of the imshow() windows\n",
    "        # When everything done, release the capture\n",
    "        print(\"Releasing camera and cv2 resources\")\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outfile = \"capture_images/live_image1.png\"\n",
    "#dam_one_shot_live_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility to click picture and save into file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_picture(file):\n",
    "    \n",
    "    try: \n",
    "\n",
    "        #Number of frames to throw away while the camera adjusts to light levels\n",
    "        ramp_frames = 30\n",
    "\n",
    "        print(\"opening camera and cv2 resources\")\n",
    "        cap = cv2.VideoCapture(camera_port)\n",
    "\n",
    "        #Check whether user selected camera is opened successfully.\n",
    "        if not (cap.isOpened()):\n",
    "            print(\"Could not open video device\")\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        cv2.imwrite(file, frame)\n",
    "    finally:        \n",
    "        # Release the camera, then close all of the imshow() windows\n",
    "        # When everything done, release the capture\n",
    "        print(\"Releasing camera and cv2 resources\")\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outfilenane = \"capture_images/test_image1.png\"\n",
    "#click_picture(outfilenane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Predict test data***"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "#data_dir = \"D:/my_learning/my_work/lab/3-capstone_driver_activity/realtest/real_drivers/\"\n",
    "data_dir = \"D:/my_learning/my_work/lab/3-capstone_driver_activity/imgs_real_drivers/set2\"\n",
    "#data_dir = \"D:/my_learning_dataset/state_farms_drivers/imgs_right/test/\"\n",
    "    \n",
    "test_imgs = os.listdir(data_dir)\n",
    "cnt = np.min([60,len(test_imgs)])\n",
    "ncol = 5\n",
    "\n",
    "if(cnt%ncol):\n",
    "    nrow = int(cnt/ncol) + 1\n",
    "else:\n",
    "    nrow = int(cnt/ncol)\n",
    "\n",
    "fig, axs = plt.subplots(nrow,ncol,figsize=(18,18))\n",
    "\n",
    "#for idx, file in enumerate(test_imgs[:cnt]):\n",
    "for idx, file in enumerate(test_imgs[:cnt]):\n",
    "    filename = '{}/{}'.format(data_dir, file)\n",
    "    img = image.load_img(filename)\n",
    "    \n",
    "    preprocessedInp = dam_read_image(filename,IMAGE_SIZE)\n",
    "    preds = trainedModel.predict(preprocessedInp)\n",
    "    pos, classname, prob = getActivity(preds, threshold=0.3)\n",
    "    label = \"{}, {:0.2f}\".format(classname, prob)\n",
    "\n",
    "    rowindex = int(idx/ncol)\n",
    "    colindex = idx%ncol\n",
    "    #print(\"{},{}\".format(rowindex,colindex))\n",
    "    axs[rowindex][colindex].imshow(img)\n",
    "    axs[rowindex][colindex].set_title(label)\n",
    "    axs[rowindex][colindex].axis('off')\n",
    "    if(idx == cnt-1):\n",
    "        break\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('results/testresult_vgg16_righthand_TB4B5.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "R8_External_Lab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
